{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate -U \n",
    "#  Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`\n",
    "#!pip install transformers -U\n",
    "#!pip install evaluate\n",
    "#!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AJkEktHMExQr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "import accelerate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQLbdsLLyZLp"
   },
   "source": [
    "## Load data into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1705055268122,
     "user": {
      "displayName": "Julian Valline",
      "userId": "16122765922860971134"
     },
     "user_tz": -60
    },
    "id": "o72xz3DNyZLq",
    "outputId": "252cb5fe-e5fb-41e6-894d-403e81752334"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['orig', 'shuffled'],\n",
       "        num_rows: 14258\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['orig', 'shuffled'],\n",
       "        num_rows: 4753\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['orig', 'shuffled'],\n",
       "        num_rows: 4753\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_length = 50\n",
    "data = datasets.load_from_disk(r'JCLS2025_submission\\gutenberg_subset\\doc_60_chunk_'+str(chunk_length)+'.hf')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_3viFX_yZLs"
   },
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1705055268123,
     "user": {
      "displayName": "Julian Valline",
      "userId": "16122765922860971134"
     },
     "user_tz": -60
    },
    "id": "J0JbLCM2yZLt",
    "outputId": "272a40ef-1b05-4f1a-b251-3045467b6cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rdpAvcrGyZLv"
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9ozGx7gyZLy"
   },
   "source": [
    "## Tokenizer (T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3M5qM5XgPqVH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"t5-large\"\n",
    "def load_tokenizer(checkpoint, model_max_length=512):\n",
    "    # set scope to global to access from anywhere\n",
    "    global tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, fn_kwargs = {\"model_max_length\": model_max_length})\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer(checkpoint, model_max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0G18o92yZL7"
   },
   "source": [
    "### Create Preprocessing function to tokenize original and transformed data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iloVJiw4yZL7"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, source_split=\"shuffled\", target_split=\"orig\", prefix=\"\", max_length=512):\n",
    "    sources = [prefix + example for example in examples[source_split]]\n",
    "    targets = [example for example in examples[target_split]]\n",
    "    model_inputs = tokenizer(sources, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "patzQcMRt055"
   },
   "source": [
    "Dorp columns that are not the current randomization rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5ukuQ3tHt05-"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e3bd9383754633bd414efa5091f9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9013 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1428b98a33a74fc39632ef9b57df6923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87249670d1054bc9bd4a9d33c0cc99e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_data(data, tokenizer, source_split, target_split, prefix, max_length):\n",
    "    tokenized_train_data = data[\"train\"].map(preprocess_function, batched=True,\n",
    "                                             fn_kwargs={\"tokenizer\": tokenizer, \"source_split\": source_split,\n",
    "                                                        \"target_split\": target_split, \"prefix\": prefix, \"max_length\": max_length})\n",
    "    tokenized_test_data = data[\"test\"].map(preprocess_function, batched=True,\n",
    "                                           fn_kwargs={\"tokenizer\": tokenizer, \"source_split\": source_split,\n",
    "                                                        \"target_split\": target_split, \"prefix\": prefix, \"max_length\": max_length})\n",
    "    tokenized_val_data = data[\"valid\"].map(preprocess_function, batched=True,\n",
    "                                                fn_kwargs={\"tokenizer\": tokenizer, \"source_split\": source_split,\n",
    "                                                        \"target_split\": target_split, \"prefix\": prefix, \"max_length\": max_length})\n",
    "\n",
    "    return tokenized_train_data, tokenized_test_data, tokenized_val_data\n",
    "    \n",
    "source_split = 'shuffled'\n",
    "target_split = 'orig'\n",
    "prefix = ''\n",
    "max_length = 512\n",
    "tokenized_train_data, tokenized_test_data, tokenized_val_data = tokenize_data(data, tokenizer, source_split, target_split, prefix, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IZQ1NIjZyZMN"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1XQzLjZByZMQ"
   },
   "outputs": [],
   "source": [
    "def load_metrics():\n",
    "    metric1 = evaluate.load(\"sacrebleu\")\n",
    "    metric2 = evaluate.load(\"wer\")\n",
    "    metric3 = evaluate.load(\"rouge\")\n",
    "    return metric1, metric2, metric3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "s0NtZ5OYyZMV"
   },
   "outputs": [],
   "source": [
    "def postprocess_text_bleu(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def postprocess_text_wer(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "    #print(\"predictions before: \", len(preds))\n",
    "    #print(\"predictions before [0]: \", len(preds[0]))\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    #print(\"predictions decoded: \", len(decoded_preds))\n",
    "    #print(\"predictions decoded [0]: \", len(decoded_preds[0]))\n",
    "\n",
    "    #print(\"labels_before: \", labels[0])\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    #print(\"pad_token_id: \", tokenizer.pad_token_id)\n",
    "    #print(\"labels: \", labels[0])\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    #print(\"decoded_labels: \", decoded_labels[0])\n",
    "\n",
    "    decoded_preds_bleu, decoded_labels_bleu = postprocess_text_bleu(decoded_preds, decoded_labels)\n",
    "    decoded_preds_wer, decoded_labels_wer = postprocess_text_wer(decoded_preds, decoded_labels)\n",
    "\n",
    "    #print(\"Len decoded preds: \", len(decoded_preds_wer))\n",
    "    #print(\"Len decoded preds [0]: \", len(decoded_preds_wer[0]))\n",
    "    #print(\"Decoded preds: \", decoded_preds_wer[0])\n",
    "    #print(\"Decoded preds_next: \", decoded_preds_wer[1])\n",
    "    #print(\"Decoded labels: \", decoded_labels_wer[0])\n",
    "\n",
    "    # load metrics\n",
    "    metric1, metric2, metric3 = load_metrics()\n",
    "\n",
    "    result1 = metric1.compute(predictions=decoded_preds_bleu, references=decoded_labels_bleu)\n",
    "    result2 = metric2.compute(predictions=decoded_preds_wer, references=decoded_labels_wer)\n",
    "    result3 = metric3.compute(predictions=decoded_preds_wer, references=decoded_labels_wer)\n",
    "    result = {\"bleu\": result1[\"score\"], \"wer\": result2, \"rougeL\": result3[\"rougeL\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO6IXaMSyZMX"
   },
   "source": [
    "# Train\n",
    "\n",
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VV9AG2d2yZMY"
   },
   "outputs": [],
   "source": [
    "#import torch.nn as nn\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "#model= nn.DataParallel(model)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cscLOLmt06m",
    "outputId": "55c9848e-c558-400a-fbe2-c085d7c131c2"
   },
   "outputs": [],
   "source": [
    "abspath = os.getcwd()\n",
    "relpath = \"data\"\n",
    "path = os.path.normpath(os.path.join(abspath, relpath))\n",
    "relpath = \"log_new\"\n",
    "logging_path = os.path.normpath(os.path.join(abspath, relpath))\n",
    "print(logging_path)\n",
    "save_path = os.path.join(abspath, os.path.join(\"models\", \"T5Large_doc_60_\"+str(chunk_length)))\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "un4dnMcfyZMa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=save_path,\n",
    "        logging_steps=500,\n",
    "        logging_dir=logging_path,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        auto_find_batch_size = True,  # autoset batch size to avoid memory issues with t5_large\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,  # decides, how many checkpoints will be kept at the end\n",
    "        #save_strategy=\"steps\",  # needs to be same as eval strategy in order to load best model\n",
    "        #save_steps=500,  # needs to be a round multiple of eval_steps (logging_steps)\n",
    "        load_best_model_at_end=True,  # keeps best model in the trainer\n",
    "        num_train_epochs=3,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,\n",
    "        push_to_hub=True,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_data,\n",
    "        eval_dataset=tokenized_test_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,  # call prepare compute metrics function to pass custom args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6762' max='6762' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6762/6762 2:19:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.394500</td>\n",
       "      <td>2.794955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.983700</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.928900</td>\n",
       "      <td>2.661429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.983500</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.833200</td>\n",
       "      <td>2.590734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.983200</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.784400</td>\n",
       "      <td>2.545350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.983000</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.716500</td>\n",
       "      <td>2.515758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982800</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.692100</td>\n",
       "      <td>2.493007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.665200</td>\n",
       "      <td>2.476178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982200</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.646100</td>\n",
       "      <td>2.460471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982100</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.635100</td>\n",
       "      <td>2.449763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982300</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.612700</td>\n",
       "      <td>2.441545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.596800</td>\n",
       "      <td>2.435825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982400</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.598000</td>\n",
       "      <td>2.431471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982400</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.589600</td>\n",
       "      <td>2.429166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982100</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\duk\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6762, training_loss=2.739321438175304, metrics={'train_runtime': 8395.4356, 'train_samples_per_second': 3.221, 'train_steps_per_second': 0.805, 'total_flos': 5.8540713836544e+16, 'train_loss': 2.739321438175304, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlRTNUcv4cZ5"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0quOhpXKzx9d"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from random import shuffle\n",
    "model_path = r'models\\T5Large_doc_60_50'\n",
    "print(model_path)\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bpxYNuf44caF",
    "outputId": "ec14be50-c2ba-4e08-8e2a-92cc8fe80775"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the a the of taking the a dust, to of talking on down had brought setting salt, salty at fan pickles rays few that the me, the tasting steaming from ground the well were young as to up their handed coming keep the treetops, looking over down women, as sun the water they they men. and'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = data[\"valid\"]['shuffled'][11]\n",
    "text = \"water from the well they had brought up on the ground to keep down the steaming dust, the rays of the setting sun coming down over the treetops, taking a fan they handed me, tasting their pickles that were as salty as salt, and looking at a few of the young women, talking to the men.\".split()\n",
    "shuffle(text)\n",
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c1oXvKcg4caG",
    "outputId": "35113be3-dfef-47a3-d00c-97eb59708e57",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=model_path, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'a few of the young men were coming down from the treetops to the ground, looking over the steaming water and taking a few salty pickles from the fan, that they had brought to me to keep the dust down, as they were talking, the sun setting on the salty'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "McZSoh_Z4caG",
    "outputId": "77d9ca8f-47aa-464e-c8aa-b8621b66edfa",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"them by the extreme tip of their steel - bound scabbards , he held them out towards the Frenchman . Chauvelin 's eyes were fixed upon him , and he from his towering height was looking down at the little sable - clad figure before him . The Terrorist seemed uncertain what to do . Though he was one of those men whom by the force of their intellect , the strength of their enthusiasm , the power of their cruelty , had built a new anarchical France , had overturned a throne and murdered a king , yet now\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"valid\"][\"orig\"][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmmQZPdnvCXQ"
   },
   "source": [
    "# Inference for the whole validation Dataset\n",
    "\n",
    "We select a subset of 5000 datapoints for inference, to cut down on computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "error",
     "timestamp": 1707296821686,
     "user": {
      "displayName": "Sarah Ackerschewski",
      "userId": "08826298637951199461"
     },
     "user_tz": -60
    },
    "id": "xV7Ei7xAoGM_",
    "outputId": "30df2e1e-acf3-42b1-c3c7-9c0592d274ce"
   },
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation\", model=model_path, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZWa1l301vG-Z"
   },
   "outputs": [],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ca78cf3293804199adce35000f5ce22e"
     ]
    },
    "id": "1u2K421f2J-n",
    "outputId": "6bc02702-60e0-4277-ebbe-f8dfee9e7eea"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "x = 0\n",
    "while x < len(val_data):\n",
    "    out = translator(val_data[x])[0]['translation_text']\n",
    "    predictions.append((source_data[x], out))\n",
    "    output_df = pd.DataFrame(predictions, columns = ['orig', 'translated'])\n",
    "    output_df.to_csv(r'JCLS2025_submission\\gutenberg_subset\\inference_results\\translated_T5Large_doc_60_100.csv', sep='\\t', index=False)\n",
    "    print(x)\n",
    "    x+=1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
